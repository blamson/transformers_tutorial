{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430c7c59-a05b-4fcd-95ec-dcaccb4f531a",
   "metadata": {},
   "source": [
    "# Transformers Walkthrough\n",
    "## Author: Brady Lamson\n",
    "\n",
    "This notebook is intended to showcase a basic workflow for finetuning a transformers LLM for `named entity recognition (NER)`. There are many resources online for this but the information I need for this specific workflow is scattered throughout many sources. As such, I thought to consolidate them into something more digestable. \n",
    "\n",
    "This notebook is a companion piece to a more proper writeup that will be hosted on my main site. I won't be including all of my code there so hopefully this notebook will serve as a good reference as you move through the walkthrough.\n",
    "\n",
    "# The Dataset\n",
    "\n",
    "We'll be using the [CONLL2003 dataset](https://huggingface.co/datasets/conll2003).\n",
    "\n",
    "> The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\n",
    "> The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2 tagging scheme, whereas the original dataset uses IOB1.\n",
    "\n",
    "This dataset conveniently contains all of the features I need for this tutorial. Obviously in the real world you'll need to put in a sizable amount of effort to getting labeled data and putting it into this format, but that's outside the scope of this tutorial. I at least hope that by showing what I'm working with and explaining what all the various features represent that this will be a less overwhelming task.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "We'll be using the huggingface `datasets` library a lot here. It's built specifically for being used with `transformers` and also comes with many datasets we can load in for demonstrations such as this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504b55d7-8d72-4869-89fe-d3273bc5e4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168775a-9183-43a8-af9f-10f1fb7d7fe1",
   "metadata": {},
   "source": [
    "What we can see here is a `DatasetDict` object. This outside object is fairly straightforward, it's a fancy dictionary that can contain any number of `Dataset` objects. For the sake of this demonstration, these `Datasets` are all different splits, or parts of the whole overall dataset.\n",
    "\n",
    "Normally you'd need to split your data and load it into a dataset dictionary yourself, so the organization of this object is helpful to develop some familiarity with so you know what you'll be building towards.\n",
    "\n",
    "First, let's check out an individual row from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a93a2fa-90df-484c-ab4d-2810390f656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c173dc90-4ad9-49af-b458-b0ecd16f4bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d1848-499f-4970-9a8d-bff1776ab7ca",
   "metadata": {},
   "source": [
    "Okay, so what do each of these features represent?\n",
    "\n",
    "- id: Simple index, easy.\n",
    "- tokens: A list containing each token making up a string. \n",
    "- pos_tags: Nothing relevant to our problem set\n",
    "- chunk_tags: Nothing relevant to our problem set\n",
    "- ner_tags: List of integers pertaining to different named entities. This is the one we care about. Index in the list corresponds to the index in tokens. So the first `ner_tag` gives the tag for the first `token`. What do the tags mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ff38e8-1ec6-429f-976e-18115f6a97fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f972a5-98c2-400c-87a8-e86640587c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
